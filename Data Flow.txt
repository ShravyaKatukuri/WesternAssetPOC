// Data Flow

Data engineering pipeline: Kafka - AWS S3 - AWS EMR - AWS S3 - AWS Redshift - AWS QuickSight

Data Science flow: AWS QuickSight - HDFS/Hive - MySQL - Tableau/Machine Learning

// Open a new terminal > create a topic > run kafka consumer of the created topic

export PATH=$PATH:/usr/hdp/current/kafka-broker/bin

zookeeper-server-start.bat config\zookeeper.properties

kafka-server-start.bat config\server.properties

kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 3 --topic transdata

kafka-console-producer --broker-list 127.0.0.1:9092 --topic=transdata

kafka-console-consumer.sh --zookeeper localhost:2181 --topic transdata --from-beginning

// Creating an S3 bucket and uploading data to it

Login to the AWS Management Console > Choose "Create Bucket" > Enter a DNS-Compliant name that's unique across all of S3 > Choose the region, Bucket Settings and Advanced Settings > Create Bucket.

// Connect with S3 bucket

Using Kafka Connect UI, we use a new connector > select S3. To remove the errors, we change the s3.region and s3.bucketname to ours.	

connector.class=io.confluent.connect.s3.S3SinkConnector
s3.region=ap-southeast-2
format.class=io.confluent.connect.s3.format.json.JsonFormat
topics.dir=topics
flush.size=1
topics=Name_of_your_topic
tasks.max=1
value.converter=org.apache.kafka.connect.storage.StringConverter
storage.class=io.confluent.connect.s3.storage.S3Storage
key.converter=org.apache.kafka.connect.storage.StringConverter
s3.bucket.name=your_bucket_name

// Read data from S3

s3://bucket-name/path-to-file-in-bucket

// Connecting to AWS EMR

Running spark job (Before running job make sure EMR Role has access to s3)

spark-submit etl.py --master yarn --deploy-mode client --driver-memory 4g --num-executors 2 --executor-memory 2g --executor-core 2

// Spark on EMR code

import configparser
from datetime import datetime
import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, col, monotonically_increasing_id
from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format, dayofweek
from pyspark.sql.types import *

config = configparser.ConfigParser()
config.read('dl.cfg')

os.environ['AWS_ACCESS_KEY_ID'] = config['AWS_ACCESS_KEY_ID']
os.environ['AWS_SECRET_ACCESS_KEY'] = config['AWS_SECRET_ACCESS_KEY']


def create_spark_session():
    spark = SparkSession \
        .builder \
        .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:2.7.0") \
        .getOrCreate()
    return spark



// Running the main() function and storing the output in S3
def main():
    spark = create_spark_session()
    input_data = "s3://westernAsset-transdata-project/"
    output_data = "s3://westernAsset-transdata-project/output/"

    process_song_data(spark, input_data, output_data)
    process_log_data(spark, input_data, output_data)


if __name__ == "__main__":
    main()


// Loading into Redshift

Setup the dwh.cfg file in the below format:
[CLUSTER]
HOST=''
DB_NAME=''
DB_USER=''
DB_PASSWORD=''
DB_PORT=5439

[IAM_ROLE]
ARN=<IAM Role arn>

[S3]
LOG_DATA='s3://westernAsset-dend/trans_data'
LOG_JSONPATH='s3://wetsernAsset-dend/trans_json_path.json'

//Creating tables in Redshift DWH

$ python create_tables.py

import configparser
import psycopg2
from sql_queries import create_table_queries, drop_table_queries


def drop_tables(cur, conn):
    for query in drop_table_queries:
        cur.execute(query)
        conn.commit()


def create_tables(cur, conn):
    for query in create_table_queries:
        cur.execute(query)
        conn.commit()


def main():
    config = configparser.ConfigParser()
    config.read('dwh.cfg')

    conn = psycopg2.connect("host={} dbname={} user={} password={} port={}".format(*config['CLUSTER'].values()))
    cur = conn.cursor()

    drop_tables(cur, conn)
    create_tables(cur, conn)

    conn.close()


if __name__ == "__main__":
    main()

//Loading Data (ETL) into Redshift

import configparser
import psycopg2
from sql_queries import copy_table_queries, insert_table_queries


def load_staging_tables(cur, conn):
    for query in copy_table_queries:
        cur.execute(query)
        conn.commit()


def insert_tables(cur, conn):
    for query in insert_table_queries:
        cur.execute(query)
        conn.commit()


def main():
    config = configparser.ConfigParser()
    config.read('dwh.cfg')

    conn = psycopg2.connect("host={} dbname={} user={} password={} port={}".format(*config['CLUSTER'].values()))
    cur = conn.cursor()
    
    load_staging_tables(cur, conn)
    insert_tables(cur, conn)

    conn.close()


if __name__ == "__main__":
    main()

//Using AWS QuickSight

Go to manage data > New dataset > S3 > give a name and Create data source > select the database > select the table > choose the fields, calculated fields and appropriate visualizations.

























